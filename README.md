# A Comparative Study of Apache Spark and Ray for Scalable Big Data Analytics and Machine Learning in Python

>> Python 2: Ray, Spark

>> Team ID: 5

This project explores a comparative study of Ray and Apache Spark, two leading Python-based frameworks for scalable big data processing and machine learning tasks. The objective is to evaluate their performance, scalability, and suitability for handling large-scale datasets that exceed main memory capacity, involving millions or billions of records. Both systems will be installed and configured on local or Okeanos-based resources, with a shared dataset loaded into each framework for consistency. A series of Python scripts will benchmark their capabilities across a variety of scenarios, including common ETL workflows and machine learning operations such as prediction, clustering, and graph analysis. By varying the number of nodes, input data size, and data types, the study aims to derive meaningful insights into the strengths and limitations of each framework, providing a comprehensive analysis of their scalability and efficiency for real-world applications.

## Install and setup

Installation and setup instructions for VMs, Hadoop, Ray and Spark on local resources, and the [assignment](https://github.com/ntua-el20439/Big-Data-HDFS-Ray-vs-Spark/blob/main/documentation/ASSIGNMENT.md) can be found in the [documentation](https://github.com/ntua-el20439/Big-Data-HDFS-Ray-vs-Spark/blob/main/documentation/) folder.

## Data gathering

Read [this](https://github.com/ntua-el20439/Big-Data-HDFS-Ray-vs-Spark/blob/main/data/README.md) to learn how to download and generate the datasets.

## Experiments

Read the [tutorial](https://github.com/ntua-el20439/Big-Data-HDFS-Ray-vs-Spark/blob/main/analysis/README.md) to learn how to run the experiments.
